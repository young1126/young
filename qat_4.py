import torch, torch.nn as nn, torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(0)

# â”€â”€â”€ 1. Util â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class BinFn(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x): return x.sign()
    @staticmethod
    def backward(ctx, g): return g
sign_ste = BinFn.apply

class FakeQuant8(nn.Module):            # Conv1ìš©
    def __init__(self):
        super().__init__()
        self.register_buffer("scale", torch.tensor(1.0))

    def forward(self, w):               # í•™ìŠµÂ·ì¶”ë¡ ìš© float ì¶œë ¥
        with torch.no_grad():
            self.scale.copy_(w.abs().max() / 127 + 1e-8)
        q = torch.clamp((w / self.scale).round(), -128, 127)
        return q * self.scale

    def quantize_to_int(self, w):       # HLSìš© int8 ì¶”ì¶œ
        with torch.no_grad():
            self.scale.copy_(w.abs().max() / 127 + 1e-8)
        return torch.clamp((w / self.scale).round(), -128, 127).to(torch.int8)

# â”€â”€â”€ 2. TeacherNet (FP32) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TeacherNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, 3)
        self.conv2 = nn.Conv2d(16, 16, 3)
        self.conv3 = nn.Conv2d(16, 32, 3)
        self.pool  = nn.AvgPool2d(2)
        self.fc    = nn.Linear(32*11*11, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = self.pool(x).flatten(1)
        return self.fc(x)

# â”€â”€â”€ 3. Binary Convolution (ì¼ë°˜ BNN) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class BConv2d(nn.Conv2d):
    """ì¼ë°˜ì ì¸ BNN: sign() ì ìš©ëœ ê°€ì¤‘ì¹˜ë¡œ convolution"""
    
    def forward(self, x):
        w_bin = sign_ste(self.weight)  # ê°€ì¤‘ì¹˜ë§Œ ì´ì§„í™”
        return F.conv2d(x, w_bin, None, self.stride, self.padding)

# â”€â”€â”€ 4. StudentQAT (ì¼ë°˜ BNN ë°©ì‹) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class StudentQAT(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, 3, bias=False)
        self.q1    = FakeQuant8()              # 8-bit quantization
        self.conv2 = BConv2d(16, 16, 3, bias=False)  # Binary weights
        self.conv3 = BConv2d(16, 32, 3, bias=False)  # Binary weights
        self.pool  = nn.AvgPool2d(2)
        self.fc    = nn.Linear(32*11*11, 10, bias=True)

    def forward(self, x):
        # Conv1: 8ë¹„íŠ¸ ì–‘ìí™” + Sign í™œì„±í™”
        x = self.q1(self.conv1(x))  # 8ë¹„íŠ¸ ì–‘ìí™”ëœ convolution
        x = sign_ste(x)             # Sign í™œì„±í™”
        
        # Conv2: Binary weights + Sign í™œì„±í™”
        x = sign_ste(self.conv2(x))
        
        # Conv3: Binary weights + Sign í™œì„±í™”  
        x = sign_ste(self.conv3(x))
        
        # Pooling + FC
        x = self.pool(x).flatten(1)
        
        # FC: Binary weights
        w_fc = sign_ste(self.fc.weight)
        return F.linear(x, w_fc, self.fc.bias)

# â”€â”€â”€ 5. KD Loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kd_loss(s, t, y, T=2.0, alpha=0.5):
    ce = F.cross_entropy(s, y)
    kd = F.kl_div(F.log_softmax(s/T, 1), F.softmax(t/T, 1),
                  reduction="batchmean") * T * T
    return alpha*kd + (1-alpha)*ce

# â”€â”€â”€ 6. Train / Eval ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_ep(stu, tea, ldr, opt, ep, T, alpha):
    stu.train(); tea.eval(); tot=hit=loss_sum=0
    for x, y in ldr:
        x, y = x.to(device), y.to(device)
        opt.zero_grad()
        with torch.no_grad(): t = tea(x)
        s = stu(x)
        loss = kd_loss(s, t, y, T, alpha)
        loss.backward(); opt.step()
        tot += y.size(0); hit += (s.argmax(1)==y).sum().item(); loss_sum += loss.item()
    print(f"Epoch {ep:02d} | loss {loss_sum/len(ldr):.4f} | acc {100*hit/tot:.2f}%")
    return loss_sum/len(ldr), 100*hit/tot

@torch.no_grad()
def eval_acc(net, ldr, tag):
    net.eval(); tot=hit=0
    for x, y in ldr:
        x, y = x.to(device), y.to(device)
        hit += (net(x).argmax(1)==y).sum().item(); tot += y.size(0)
    acc = 100*hit/tot
    print(f"{tag} acc {acc:.2f}%"); return acc

# â”€â”€â”€ 7. ë°ì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def binarize_img(t): return (t>0.5).float()*2-1
tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(binarize_img)])
def get_ld(bs=128):
    tr = datasets.MNIST(".", True , download=True, transform=tfm)
    te = datasets.MNIST(".", False, download=True, transform=tfm)
    return (DataLoader(tr, bs, True , num_workers=2, pin_memory=True),
            DataLoader(te, bs, False, num_workers=2, pin_memory=True))

# â”€â”€â”€ 8. ê²€ì¦ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pack_bits(img):
    bits = (img > 0.5).to(torch.uint8)
    out  = torch.empty((img.size(0), 98), dtype=torch.uint8)
    for i in range(img.size(0)):
        out[i] = torch.from_numpy(np.packbits(bits[i].view(-1).cpu().numpy()))
    return out

def unpack_to_pm1(packed):
    imgs = []
    for row in packed:
        bits = np.unpackbits(row.cpu().numpy())[:784].astype(np.float32)
        imgs.append(torch.tensor(bits.reshape(1,28,28))*2 - 1)
    return torch.stack(imgs).to(device)

@torch.no_grad()
def verify_model(model, loader):
    """HLS ë°©ì‹ ê²€ì¦: ë¹„íŠ¸íŒ¨í‚¹ ì…ë ¥ ì‚¬ìš©"""
    model.eval()
    
    hit = tot = 0
    for x, y in loader:
        pk   = pack_bits(x)
        x_pm = unpack_to_pm1(pk)
        y    = y.to(device)

        logits = model(x_pm)
        pred = logits.argmax(1)
        hit += (pred == y).sum().item()
        tot += y.size(0)

    acc = 100*hit/tot
    print(f"ğŸ“Š HLS style acc = {acc:.2f}%")
    return acc

# â”€â”€â”€ 9. ì˜¬ë°”ë¥¸ ê°€ì¤‘ì¹˜ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@torch.no_grad()
def save_weights_correctly(student, save_path):
    """HLSì™€ ì™„ì „íˆ ë§¤ì¹­ë˜ëŠ” ê°€ì¤‘ì¹˜ ì €ì¥"""
    student.eval()
    
    print("\nğŸ”§ ê°€ì¤‘ì¹˜ ì €ì¥ ì¤‘...")
    
    # 1. Conv1: 8ë¹„íŠ¸ ì–‘ìí™” + ìŠ¤ì¼€ì¼
    conv1_w = student.conv1.weight.data
    conv1_quantized = student.q1.quantize_to_int(conv1_w).cpu().numpy()
    conv1_scale = student.q1.scale.cpu().item()
    
    print(f"Conv1 ì›ë³¸ ë²”ìœ„: [{conv1_w.min():.3f}, {conv1_w.max():.3f}]")
    print(f"Conv1 ì–‘ìí™” ë²”ìœ„: [{conv1_quantized.min()}, {conv1_quantized.max()}]")
    print(f"Conv1 ìŠ¤ì¼€ì¼: {conv1_scale:.6f}")
    
    # 2. Conv2, Conv3, FC: 1ë¹„íŠ¸ ì´ì§„í™”
    def to_binary_correct(weight, name):
        """ì˜¬ë°”ë¥¸ ì´ì§„í™”: sign() â†’ {-1,+1} â†’ {0,1}"""
        with torch.no_grad():
            # Step 1: ê°€ì¤‘ì¹˜ì— sign() ì ìš©
            sign_w = weight.sign()
            unique_sign = torch.unique(sign_w)
            print(f"{name} sign ìœ ë‹ˆí¬ ê°’: {unique_sign}")
            
            # Step 2: {-1,+1} â†’ {0,1} ë³€í™˜
            binary_w = ((sign_w + 1) / 2).cpu().numpy().astype(np.uint8)
            unique_bin = np.unique(binary_w)
            print(f"{name} ì´ì§„í™” ìœ ë‹ˆí¬ ê°’: {unique_bin}")
            
            # Step 3: ë¶„í¬ í™•ì¸
            ratio_1 = np.mean(binary_w == 1)
            ratio_0 = np.mean(binary_w == 0)
            print(f"{name} ë¶„í¬ - 0: {ratio_0:.3f}, 1: {ratio_1:.3f}")
            
            # ë¬¸ì œ ì²´í¬
            if not np.array_equal(unique_bin, [0, 1]):
                print(f"âš ï¸  {name} ì´ì§„í™” ì‹¤íŒ¨! ìœ ë‹ˆí¬ ê°’: {unique_bin}")
            
            return binary_w
    
    conv2_bin = to_binary_correct(student.conv2.weight, "Conv2")
    conv3_bin = to_binary_correct(student.conv3.weight, "Conv3") 
    fc_w_bin = to_binary_correct(student.fc.weight, "FC")
    
    # 3. FC bias: float32
    fc_bias = student.fc.bias.cpu().numpy().astype(np.float32)
    print(f"FC bias ë²”ìœ„: [{fc_bias.min():.3f}, {fc_bias.max():.3f}]")
    print(f"FC bias í‰ê· : {fc_bias.mean():.3f}, í‘œì¤€í¸ì°¨: {fc_bias.std():.3f}")
    
    # 4. ê°€ì¤‘ì¹˜ í†µê³„ í™•ì¸
    print("\nğŸ“Š ê°€ì¤‘ì¹˜ í†µê³„:")
    print(f"Conv1 non-zero ë¹„ìœ¨: {np.mean(conv1_quantized != 0):.3f}")
    print(f"Conv2 1ì˜ ë¹„ìœ¨: {np.mean(conv2_bin == 1):.3f}")
    print(f"Conv3 1ì˜ ë¹„ìœ¨: {np.mean(conv3_bin == 1):.3f}")
    print(f"FC 1ì˜ ë¹„ìœ¨: {np.mean(fc_w_bin == 1):.3f}")
    
    # 5. ì €ì¥
    weights_dict = {
        "conv1": conv1_quantized,
        "conv1_scale": conv1_scale,
        "conv2": conv2_bin,
        "conv3": conv3_bin,
        "fc_w": fc_w_bin,
        "fc_b": fc_bias
    }
    
    np.save(save_path, weights_dict)
    print(f"âœ… ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    # 6. ì €ì¥ ê²€ì¦
    print("\nğŸ” ì €ì¥ëœ ê°€ì¤‘ì¹˜ ê²€ì¦:")
    saved = np.load(save_path, allow_pickle=True).item()
    for key, value in saved.items():
        if isinstance(value, np.ndarray):
            print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
            print(f"    ë²”ìœ„: [{value.min()}, {value.max()}]")
            if key in ['conv2', 'conv3', 'fc_w']:
                unique_vals = np.unique(value)
                print(f"    ìœ ë‹ˆí¬ ê°’: {unique_vals}")
                if not np.array_equal(unique_vals, [0, 1]):
                    print(f"    âŒ ì˜ˆìƒê³¼ ë‹¤ë¦„! 0,1ì´ì–´ì•¼ í•¨")
                else:
                    print(f"    âœ… ì˜¬ë°”ë¥¸ ì´ì§„ ê°’")
        else:
            print(f"  {key}: {value}")
        print()
    
    return weights_dict

# â”€â”€â”€ 10. ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    tr_loader, te_loader = get_ld()

    # â‘  Teacher í•™ìŠµ
    print("ğŸ“ Teacher í•™ìŠµ ì‹œì‘")
    teacher = TeacherNet().to(device)
    opt_t = torch.optim.Adam(teacher.parameters(), 1e-3)
    
    for ep in range(10):
        loss, acc = train_ep(teacher, teacher, tr_loader, opt_t, ep, 1, 0)
        
        # í•™ìŠµì´ ì œëŒ€ë¡œ ì•ˆë˜ë©´ ì¡°ê¸° ì¢…ë£Œ
        if ep > 5 and acc < 50:
            print("âš ï¸  Teacher í•™ìŠµì´ ì œëŒ€ë¡œ ì•ˆë©ë‹ˆë‹¤. Learning rate ì¡°ì • í•„ìš”.")
            break
    
    teacher_acc = eval_acc(teacher, te_loader, "Teacher")
    
    # Teacher ì„±ëŠ¥ ì²´í¬
    if teacher_acc < 85:
        print(f"âš ï¸  Teacher ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤: {teacher_acc:.1f}%")
        print("ë” í•™ìŠµí•˜ê±°ë‚˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ì„¸ìš”.")
        # ê·¸ë˜ë„ ê³„ì† ì§„í–‰
    
    # â‘¡ Student í•™ìŠµ
    print("\nğŸ“ Student QAT í•™ìŠµ ì‹œì‘")
    student = StudentQAT().to(device)
    opt_s = torch.optim.Adam(student.parameters(), 1e-3)
    
    best_acc = 0
    best_epoch = 0
    
    for ep in range(40):
        loss, acc = train_ep(student, teacher, tr_loader, opt_s, ep, 1.0, 0.3)
        val_acc = eval_acc(student, te_loader, "Student")
        
        if val_acc > best_acc:
            best_acc = val_acc
            best_epoch = ep
        
        # í•™ìŠµ ì§„í–‰ ì²´í¬
        if ep > 15 and val_acc < 20:
            print("âŒ Student í•™ìŠµì´ ì‹¤íŒ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
            print("íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤:")
            print("  - Learning rate ê°ì†Œ (1e-4)")
            print("  - Temperature ì¦ê°€ (5.0)")
            print("  - Alpha ì¡°ì • (0.8)")
            break
        
        # ê²€ì¦
        if ep % 10 == 0 or ep == 39:
            print(f"\n--- Epoch {ep} ê²€ì¦ ---")
            hls_acc = verify_model(student, te_loader)
            print(f"ì¼ë°˜ ì •í™•ë„: {val_acc:.2f}%")
            print(f"HLS ì •í™•ë„: {hls_acc:.2f}%")
            print(f"ì°¨ì´: {abs(val_acc - hls_acc):.2f}%")
            print("--- ê²€ì¦ ì™„ë£Œ ---\n")
    
    print(f"âœ… Best Student acc: {best_acc:.2f}% (Epoch {best_epoch})")
    
    # â‘¢ ìµœì¢… ì„±ëŠ¥ ì²´í¬
    if best_acc < 60:
        print("âŒ Student ì„±ëŠ¥ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤!")
        print("ê°€ì¤‘ì¹˜ ì €ì¥ì„ ì§„í–‰í•˜ì§€ë§Œ, í•™ìŠµì„ ë‹¤ì‹œ í•´ë³´ì„¸ìš”.")
    
    # â‘£ ê°€ì¤‘ì¹˜ ì €ì¥
    print("\nğŸ’¾ ê°€ì¤‘ì¹˜ ì €ì¥")
    save_path = "/home/jinseopalang/young/hls_friendly_weights_fixed.npy"
    weights = save_weights_correctly(student, save_path)
    
    # â‘¤ ìµœì¢… ê²€ì¦
    print("\nğŸ¯ ìµœì¢… ê²€ì¦:")
    final_normal_acc = eval_acc(student, te_loader, "Final")
    final_hls_acc = verify_model(student, te_loader)
    
    print(f"\nğŸ“ˆ ìµœì¢… ê²°ê³¼:")
    print(f"ì¼ë°˜ ì •í™•ë„: {final_normal_acc:.2f}%")
    print(f"HLS ì •í™•ë„: {final_hls_acc:.2f}%")
    print(f"ì°¨ì´: {abs(final_normal_acc - final_hls_acc):.2f}%")
    
    if final_hls_acc > 60:
        print(f"âœ… í•™ìŠµ ì„±ê³µ!")
        print(f"ğŸ’¾ ê°€ì¤‘ì¹˜ íŒŒì¼: {save_path}")
    else:
        print(f"âš ï¸  HLS ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤: {final_hls_acc:.2f}%")
        print("ê°€ì¤‘ì¹˜ëŠ” ì €ì¥ë˜ì—ˆì§€ë§Œ ì¬í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤.")